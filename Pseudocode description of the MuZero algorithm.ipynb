{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"https://arxiv.org/src/1911.08265v2/anc/pseudocode.py\"\n",
    "# Lint as: python3\n",
    "\"\"\"Pseudocode description of the MuZero algorithm.\"\"\"\n",
    "# pylint: disable=unused-argument\n",
    "# pylint: disable=missing-docstring\n",
    "# pylint: disable=g-explicit-length-test\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import typing\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "##########################\n",
    "####### Helpers ##########\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float('inf')\n",
    "\n",
    "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "  \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
    "\n",
    "  def __init__(self, known_bounds: Optional[KnownBounds]):\n",
    "    self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "    self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "  def update(self, value: float):\n",
    "    self.maximum = max(self.maximum, value)\n",
    "    self.minimum = min(self.minimum, value)\n",
    "\n",
    "  def normalize(self, value: float) -> float:\n",
    "    if self.maximum > self.minimum:\n",
    "      # We normalize only when we have set the maximum and minimum values.\n",
    "      return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "    return value\n",
    "\n",
    "\n",
    "class MuZeroConfig(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               action_space_size: int,\n",
    "               max_moves: int,\n",
    "               discount: float,\n",
    "               dirichlet_alpha: float,\n",
    "               num_simulations: int,\n",
    "               batch_size: int,\n",
    "               td_steps: int,\n",
    "               num_actors: int,\n",
    "               lr_init: float,\n",
    "               lr_decay_steps: float,\n",
    "               visit_softmax_temperature_fn,\n",
    "               known_bounds: Optional[KnownBounds] = None):\n",
    "    ### Self-Play\n",
    "    self.action_space_size = action_space_size\n",
    "    self.num_actors = num_actors\n",
    "\n",
    "    self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
    "    self.max_moves = max_moves\n",
    "    self.num_simulations = num_simulations\n",
    "    self.discount = discount\n",
    "\n",
    "    # Root prior exploration noise.\n",
    "    self.root_dirichlet_alpha = dirichlet_alpha\n",
    "    self.root_exploration_fraction = 0.25\n",
    "\n",
    "    # UCB formula\n",
    "    self.pb_c_base = 19652\n",
    "    self.pb_c_init = 1.25\n",
    "\n",
    "    # If we already have some information about which values occur in the\n",
    "    # environment, we can use them to initialize the rescaling.\n",
    "    # This is not strictly necessary, but establishes identical behaviour to\n",
    "    # AlphaZero in board games.\n",
    "    self.known_bounds = known_bounds\n",
    "\n",
    "    ### Training\n",
    "    self.training_steps = int(1000e3)\n",
    "    self.checkpoint_interval = int(1e3)\n",
    "    self.window_size = int(1e6)\n",
    "    self.batch_size = batch_size\n",
    "    self.num_unroll_steps = 5\n",
    "    self.td_steps = td_steps\n",
    "\n",
    "    self.weight_decay = 1e-4\n",
    "    self.momentum = 0.9\n",
    "\n",
    "    # Exponential learning rate schedule\n",
    "    self.lr_init = lr_init\n",
    "    self.lr_decay_rate = 0.1\n",
    "    self.lr_decay_steps = lr_decay_steps\n",
    "\n",
    "  def new_game(self):\n",
    "    return Game(self.action_space_size, self.discount)\n",
    "\n",
    "\n",
    "def make_board_game_config(action_space_size: int, max_moves: int,\n",
    "                           dirichlet_alpha: float,\n",
    "                           lr_init: float) -> MuZeroConfig:\n",
    "\n",
    "  def visit_softmax_temperature(num_moves, training_steps):\n",
    "    if num_moves < 30:\n",
    "      return 1.0\n",
    "    else:\n",
    "      return 0.0  # Play according to the max.\n",
    "\n",
    "  return MuZeroConfig(\n",
    "      action_space_size=action_space_size,\n",
    "      max_moves=max_moves,\n",
    "      discount=1.0,\n",
    "      dirichlet_alpha=dirichlet_alpha,\n",
    "      num_simulations=800,\n",
    "      batch_size=2048,\n",
    "      td_steps=max_moves,  # Always use Monte Carlo return.\n",
    "      num_actors=3000,\n",
    "      lr_init=lr_init,\n",
    "      lr_decay_steps=400e3,\n",
    "      visit_softmax_temperature_fn=visit_softmax_temperature,\n",
    "      known_bounds=KnownBounds(-1, 1))\n",
    "\n",
    "\n",
    "def make_go_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=362, max_moves=722, dirichlet_alpha=0.03, lr_init=0.01)\n",
    "\n",
    "\n",
    "def make_chess_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=4672, max_moves=512, dirichlet_alpha=0.3, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_shogi_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=11259, max_moves=512, dirichlet_alpha=0.15, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_atari_config() -> MuZeroConfig:\n",
    "\n",
    "  def visit_softmax_temperature(num_moves, training_steps):\n",
    "    if training_steps < 500e3:\n",
    "      return 1.0\n",
    "    elif training_steps < 750e3:\n",
    "      return 0.5\n",
    "    else:\n",
    "      return 0.25\n",
    "\n",
    "  return MuZeroConfig(\n",
    "      action_space_size=18,\n",
    "      max_moves=27000,  # Half an hour at action repeat 4.\n",
    "      discount=0.997,\n",
    "      dirichlet_alpha=0.25,\n",
    "      num_simulations=50,\n",
    "      batch_size=1024,\n",
    "      td_steps=10,\n",
    "      num_actors=350,\n",
    "      lr_init=0.05,\n",
    "      lr_decay_steps=350e3,\n",
    "      visit_softmax_temperature_fn=visit_softmax_temperature)\n",
    "\n",
    "\n",
    "class Action(object):\n",
    "\n",
    "  def __init__(self, index: int):\n",
    "    self.index = index\n",
    "\n",
    "  def __hash__(self):\n",
    "    return self.index\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    return self.index == other.index\n",
    "\n",
    "  def __gt__(self, other):\n",
    "    return self.index > other.index\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "  pass\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "  def __init__(self, prior: float):\n",
    "    self.visit_count = 0\n",
    "    self.to_play = -1\n",
    "    self.prior = prior\n",
    "    self.value_sum = 0\n",
    "    self.children = {}\n",
    "    self.hidden_state = None\n",
    "    self.reward = 0\n",
    "\n",
    "  def expanded(self) -> bool:\n",
    "    return len(self.children) > 0\n",
    "\n",
    "  def value(self) -> float:\n",
    "    if self.visit_count == 0:\n",
    "      return 0\n",
    "    return self.value_sum / self.visit_count\n",
    "\n",
    "\n",
    "class ActionHistory(object):\n",
    "  \"\"\"Simple history container used inside the search.\n",
    "\n",
    "  Only used to keep track of the actions executed.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, history: List[Action], action_space_size: int):\n",
    "    self.history = list(history)\n",
    "    self.action_space_size = action_space_size\n",
    "\n",
    "  def clone(self):\n",
    "    return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "  def add_action(self, action: Action):\n",
    "    self.history.append(action)\n",
    "\n",
    "  def last_action(self) -> Action:\n",
    "    return self.history[-1]\n",
    "\n",
    "  def action_space(self) -> List[Action]:\n",
    "    return [Action(i) for i in range(self.action_space_size)]\n",
    "\n",
    "  def to_play(self) -> Player:\n",
    "    return Player()\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "  \"\"\"The environment MuZero is interacting with.\"\"\"\n",
    "\n",
    "  def step(self, action):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Game(object):\n",
    "  \"\"\"A single episode of interaction with the environment.\"\"\"\n",
    "\n",
    "  def __init__(self, action_space_size: int, discount: float):\n",
    "    self.environment = Environment()  # Game specific environment.\n",
    "    self.history = []\n",
    "    self.rewards = []\n",
    "    self.child_visits = []\n",
    "    self.root_values = []\n",
    "    self.action_space_size = action_space_size\n",
    "    self.discount = discount\n",
    "\n",
    "  def terminal(self) -> bool:\n",
    "    # Game specific termination rules.\n",
    "    pass\n",
    "\n",
    "  def legal_actions(self) -> List[Action]:\n",
    "    # Game specific calculation of legal actions.\n",
    "    return []\n",
    "\n",
    "  def apply(self, action: Action):\n",
    "    reward = self.environment.step(action)\n",
    "    self.rewards.append(reward)\n",
    "    self.history.append(action)\n",
    "\n",
    "  def store_search_statistics(self, root: Node):\n",
    "    sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "    action_space = (Action(index) for index in range(self.action_space_size))\n",
    "    self.child_visits.append([\n",
    "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
    "        for a in action_space\n",
    "    ])\n",
    "    self.root_values.append(root.value())\n",
    "\n",
    "  def make_image(self, state_index: int):\n",
    "    # Game specific feature planes.\n",
    "    return []\n",
    "\n",
    "  def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,\n",
    "                  to_play: Player):\n",
    "    # The value target is the discounted root value of the search tree N steps\n",
    "    # into the future, plus the discounted sum of all rewards until then.\n",
    "    targets = []\n",
    "    for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
    "      bootstrap_index = current_index + td_steps\n",
    "      if bootstrap_index < len(self.root_values):\n",
    "        value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
    "      else:\n",
    "        value = 0\n",
    "\n",
    "      for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
    "        value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
    "\n",
    "      # For simplicity the network always predicts the most recently received\n",
    "      # reward, even for the initial representation network where we already\n",
    "      # know this reward.\n",
    "      if current_index > 0 and current_index <= len(self.rewards):\n",
    "        last_reward = self.rewards[current_index - 1]\n",
    "      else:\n",
    "        last_reward = 0\n",
    "\n",
    "      if current_index < len(self.root_values):\n",
    "        targets.append((value, last_reward, self.child_visits[current_index]))\n",
    "      else:\n",
    "        # States past the end of games are treated as absorbing states.\n",
    "        targets.append((0, last_reward, []))\n",
    "    return targets\n",
    "\n",
    "  def to_play(self) -> Player:\n",
    "    return Player()\n",
    "\n",
    "  def action_history(self) -> ActionHistory:\n",
    "    return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, config: MuZeroConfig):\n",
    "    self.window_size = config.window_size\n",
    "    self.batch_size = config.batch_size\n",
    "    self.buffer = []\n",
    "\n",
    "  def save_game(self, game):\n",
    "    if len(self.buffer) > self.window_size:\n",
    "      self.buffer.pop(0)\n",
    "    self.buffer.append(game)\n",
    "\n",
    "  def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
    "    games = [self.sample_game() for _ in range(self.batch_size)]\n",
    "    game_pos = [(g, self.sample_position(g)) for g in games]\n",
    "    return [(g.make_image(i), g.history[i:i + num_unroll_steps],\n",
    "             g.make_target(i, num_unroll_steps, td_steps, g.to_play()))\n",
    "            for (g, i) in game_pos]\n",
    "\n",
    "  def sample_game(self) -> Game:\n",
    "    # Sample game from buffer either uniformly or according to some priority.\n",
    "    return self.buffer[0]\n",
    "\n",
    "  def sample_position(self, game) -> int:\n",
    "    # Sample position from game either uniformly or according to some priority.\n",
    "    return -1\n",
    "\n",
    "\n",
    "class NetworkOutput(typing.NamedTuple):\n",
    "  value: float\n",
    "  reward: float\n",
    "  policy_logits: Dict[Action, float]\n",
    "  hidden_state: List[float]\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "  def initial_inference(self, image) -> NetworkOutput:\n",
    "    # representation + prediction function\n",
    "    return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "  def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "    # dynamics + prediction function\n",
    "    return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "  def get_weights(self):\n",
    "    # Returns the weights of this network.\n",
    "    return []\n",
    "\n",
    "  def training_steps(self) -> int:\n",
    "    # How many steps / batches the network has been trained for.\n",
    "    return 0\n",
    "\n",
    "\n",
    "class SharedStorage(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._networks = {}\n",
    "\n",
    "  def latest_network(self) -> Network:\n",
    "    if self._networks:\n",
    "      return self._networks[max(self._networks.keys())]\n",
    "    else:\n",
    "      # policy -> uniform, value -> 0, reward -> 0\n",
    "      return make_uniform_network()\n",
    "\n",
    "  def save_network(self, step: int, network: Network):\n",
    "    self._networks[step] = network\n",
    "\n",
    "\n",
    "##### End Helpers ########\n",
    "##########################\n",
    "\n",
    "\n",
    "# MuZero training is split into two independent parts: Network training and\n",
    "# self-play data generation.\n",
    "# These two parts only communicate by transferring the latest network checkpoint\n",
    "# from the training to the self-play, and the finished games from the self-play\n",
    "# to the training.\n",
    "def muzero(config: MuZeroConfig):\n",
    "  storage = SharedStorage()\n",
    "  replay_buffer = ReplayBuffer(config)\n",
    "\n",
    "  for _ in range(config.num_actors):\n",
    "    launch_job(run_selfplay, config, storage, replay_buffer)\n",
    "\n",
    "  train_network(config, storage, replay_buffer)\n",
    "\n",
    "  return storage.latest_network()\n",
    "\n",
    "\n",
    "##################################\n",
    "####### Part 1: Self-Play ########\n",
    "\n",
    "\n",
    "# Each self-play job is independent of all others; it takes the latest network\n",
    "# snapshot, produces a game and makes it available to the training job by\n",
    "# writing it to a shared replay buffer.\n",
    "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
    "                 replay_buffer: ReplayBuffer):\n",
    "  while True:\n",
    "    network = storage.latest_network()\n",
    "    game = play_game(config, network)\n",
    "    replay_buffer.save_game(game)\n",
    "\n",
    "\n",
    "# Each game is produced by starting at the initial board position, then\n",
    "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
    "# of the game is reached.\n",
    "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
    "  game = config.new_game()\n",
    "\n",
    "  while not game.terminal() and len(game.history) < config.max_moves:\n",
    "    # At the root of the search tree we use the representation function to\n",
    "    # obtain a hidden state given the current observation.\n",
    "    root = Node(0)\n",
    "    current_observation = game.make_image(-1)\n",
    "    expand_node(root, game.to_play(), game.legal_actions(),\n",
    "                network.initial_inference(current_observation))\n",
    "    add_exploration_noise(config, root)\n",
    "\n",
    "    # We then run a Monte Carlo Tree Search using only action sequences and the\n",
    "    # model learned by the network.\n",
    "    run_mcts(config, root, game.action_history(), network)\n",
    "    action = select_action(config, len(game.history), root, network)\n",
    "    game.apply(action)\n",
    "    game.store_search_statistics(root)\n",
    "  return game\n",
    "\n",
    "\n",
    "# Core Monte Carlo Tree Search algorithm.\n",
    "# To decide on an action, we run N simulations, always starting at the root of\n",
    "# the search tree and traversing the tree according to the UCB formula until we\n",
    "# reach a leaf node.\n",
    "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
    "             network: Network):\n",
    "  min_max_stats = MinMaxStats(config.known_bounds)\n",
    "\n",
    "  for _ in range(config.num_simulations):\n",
    "    history = action_history.clone()\n",
    "    node = root\n",
    "    search_path = [node]\n",
    "\n",
    "    while node.expanded():\n",
    "      action, node = select_child(config, node, min_max_stats)\n",
    "      history.add_action(action)\n",
    "      search_path.append(node)\n",
    "\n",
    "    # Inside the search tree we use the dynamics function to obtain the next\n",
    "    # hidden state given an action and the previous hidden state.\n",
    "    parent = search_path[-2]\n",
    "    network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                 history.last_action())\n",
    "    expand_node(node, history.to_play(), history.action_space(), network_output)\n",
    "\n",
    "    backpropagate(search_path, network_output.value, history.to_play(),\n",
    "                  config.discount, min_max_stats)\n",
    "\n",
    "\n",
    "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
    "                  network: Network):\n",
    "  visit_counts = [\n",
    "      (child.visit_count, action) for action, child in node.children.items()\n",
    "  ]\n",
    "  t = config.visit_softmax_temperature_fn(\n",
    "      num_moves=num_moves, training_steps=network.training_steps())\n",
    "  _, action = softmax_sample(visit_counts, t)\n",
    "  return action\n",
    "\n",
    "\n",
    "# Select the child with the highest UCB score.\n",
    "def select_child(config: MuZeroConfig, node: Node,\n",
    "                 min_max_stats: MinMaxStats):\n",
    "  _, action, child = max(\n",
    "      (ucb_score(config, node, child, min_max_stats), action,\n",
    "       child) for action, child in node.children.items())\n",
    "  return action, child\n",
    "\n",
    "\n",
    "# The score for a node is based on its value, plus an exploration bonus based on\n",
    "# the prior.\n",
    "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\n",
    "              min_max_stats: MinMaxStats) -> float:\n",
    "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
    "                  config.pb_c_base) + config.pb_c_init\n",
    "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "  prior_score = pb_c * child.prior\n",
    "  if child.visit_count > 0:\n",
    "    value_score = child.reward + config.discount * min_max_stats.normalize(\n",
    "        child.value())\n",
    "  else:\n",
    "    value_score = 0\n",
    "  return prior_score + value_score\n",
    "\n",
    "\n",
    "# We expand a node using the value, reward and policy prediction obtained from\n",
    "# the neural network.\n",
    "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
    "                network_output: NetworkOutput):\n",
    "  node.to_play = to_play\n",
    "  node.hidden_state = network_output.hidden_state\n",
    "  node.reward = network_output.reward\n",
    "  policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "  policy_sum = sum(policy.values())\n",
    "  for action, p in policy.items():\n",
    "    node.children[action] = Node(p / policy_sum)\n",
    "\n",
    "\n",
    "# At the end of a simulation, we propagate the evaluation all the way up the\n",
    "# tree to the root.\n",
    "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
    "                  discount: float, min_max_stats: MinMaxStats):\n",
    "  for node in reversed(search_path):\n",
    "    node.value_sum += value if node.to_play == to_play else -value\n",
    "    node.visit_count += 1\n",
    "    min_max_stats.update(node.value())\n",
    "\n",
    "    value = node.reward + discount * value\n",
    "\n",
    "\n",
    "# At the start of each search, we add dirichlet noise to the prior of the root\n",
    "# to encourage the search to explore new actions.\n",
    "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "  actions = list(node.children.keys())\n",
    "  noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "  frac = config.root_exploration_fraction\n",
    "  for a, n in zip(actions, noise):\n",
    "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "\n",
    "######### End Self-Play ##########\n",
    "##################################\n",
    "\n",
    "##################################\n",
    "####### Part 2: Training #########\n",
    "\n",
    "\n",
    "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
    "                  replay_buffer: ReplayBuffer):\n",
    "  network = Network()\n",
    "  learning_rate = config.lr_init * config.lr_decay_rate**(\n",
    "      tf.train.get_global_step() / config.lr_decay_steps)\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)\n",
    "\n",
    "  for i in range(config.training_steps):\n",
    "    if i % config.checkpoint_interval == 0:\n",
    "      storage.save_network(i, network)\n",
    "    batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
    "    update_weights(optimizer, network, batch, config.weight_decay)\n",
    "  storage.save_network(config.training_steps, network)\n",
    "\n",
    "\n",
    "def scale_gradient(tensor, scale):\n",
    "  \"\"\"Scales the gradient for the backward pass.\"\"\"\n",
    "  return tensor * scale + tf.stop_gradient(tensor) * (1 - scale)\n",
    "\n",
    "\n",
    "def update_weights(optimizer: tf.train.Optimizer, network: Network, batch,\n",
    "                   weight_decay: float):\n",
    "  loss = 0\n",
    "  for image, actions, targets in batch:\n",
    "    # Initial step, from the real observation.\n",
    "    value, reward, policy_logits, hidden_state = network.initial_inference(\n",
    "        image)\n",
    "    predictions = [(1.0, value, reward, policy_logits)]\n",
    "\n",
    "    # Recurrent steps, from action and previous hidden state.\n",
    "    for action in actions:\n",
    "      value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
    "          hidden_state, action)\n",
    "      predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
    "\n",
    "      hidden_state = scale_gradient(hidden_state, 0.5)\n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "      gradient_scale, value, reward, policy_logits = prediction\n",
    "      target_value, target_reward, target_policy = target\n",
    "\n",
    "      l = (\n",
    "          scalar_loss(value, target_value) +\n",
    "          scalar_loss(reward, target_reward) +\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "              logits=policy_logits, labels=target_policy))\n",
    "\n",
    "      loss += scale_gradient(l, gradient_scale)\n",
    "\n",
    "  for weights in network.get_weights():\n",
    "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "  optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "def scalar_loss(prediction, target) -> float:\n",
    "  # MSE in board games, cross entropy between categorical values in Atari.\n",
    "  return -1\n",
    "\n",
    "######### End Training ###########\n",
    "##################################\n",
    "\n",
    "################################################################################\n",
    "############################# End of pseudocode ################################\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Stubs to make the typechecker happy.\n",
    "def softmax_sample(distribution, temperature: float):\n",
    "  return 0, 0\n",
    "\n",
    "\n",
    "def launch_job(f, *args):\n",
    "  f(*args)\n",
    "\n",
    "\n",
    "def make_uniform_network():\n",
    "  return Network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
